{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49595745",
   "metadata": {},
   "source": [
    "# BERT - Is Solved? \n",
    "\n",
    "Can we use BERT to determine if a given scramble + solution sequence will result in a solved cube? \n",
    "\n",
    "Short answer - Kind of! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1719f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import datasets\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"rubiks-bert\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MAX_LENGTH = 140\n",
    "\n",
    "dataset = datasets.load_from_disk(\"../rubiks-is-solved-dataset\")\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../rubiks-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db9001fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(args):\n",
    "    scramble = args[\"scramble\"]\n",
    "    solve = args[\"solve\"]\n",
    "    is_solved = args[\"is_solved\"]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        text=scramble,\n",
    "        text_pair=solve,\n",
    "        is_split_into_words=True,\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    tokenized[\"labels\"] = is_solved\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "cols_to_remove = [\"scramble\", \"solve\", \"is_solved\"]\n",
    "processed_dataset = dataset.map(process, batched=True, remove_columns=cols_to_remove)\n",
    "\n",
    "train = processed_dataset[\"train\"]\n",
    "test = processed_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa62f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=512,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=1024,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=MAX_LENGTH,\n",
    "    type_vocab_size=2,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-12,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    position_embedding_type=\"absolute\",\n",
    "    use_cache=True,\n",
    "    classifier_dropout=None,\n",
    "    num_labels=2,\n",
    "    id2label={1: \"solved\", 0: \"not-solved\"},\n",
    ")\n",
    "model = BertForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b43abe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlainon\u001b[0m (\u001b[33mhenry-williams\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/henrywilliams/Documents/programming/python/ai/rubiks/notebooks/wandb/run-20250917_115616-vhfwltgz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/henry-williams/rubiks-bert/runs/vhfwltgz' target=\"_blank\">expert-dust-2</a></strong> to <a href='https://wandb.ai/henry-williams/rubiks-bert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/henry-williams/rubiks-bert' target=\"_blank\">https://wandb.ai/henry-williams/rubiks-bert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/henry-williams/rubiks-bert/runs/vhfwltgz' target=\"_blank\">https://wandb.ai/henry-williams/rubiks-bert/runs/vhfwltgz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15690' max='15690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15690/15690 11:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.215700</td>\n",
       "      <td>0.184641</td>\n",
       "      <td>0.932409</td>\n",
       "      <td>0.933429</td>\n",
       "      <td>0.932409</td>\n",
       "      <td>0.932867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.187300</td>\n",
       "      <td>0.190034</td>\n",
       "      <td>0.939484</td>\n",
       "      <td>0.940220</td>\n",
       "      <td>0.939484</td>\n",
       "      <td>0.939817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.160700</td>\n",
       "      <td>0.191026</td>\n",
       "      <td>0.942256</td>\n",
       "      <td>0.942686</td>\n",
       "      <td>0.942256</td>\n",
       "      <td>0.942457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (rubiks-bert/checkpoint-5230)... Done. 0.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15690, training_loss=0.20078107871547332, metrics={'train_runtime': 672.7743, 'train_samples_per_second': 186.553, 'train_steps_per_second': 23.321, 'total_flos': 914665565298240.0, 'train_loss': 0.20078107871547332, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"weighted\"\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./rubiks-bert\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    torch_empty_cache_steps=500,\n",
    "    save_strategy=\"best\",\n",
    "    report_to=\"wandb\",\n",
    "    auto_find_batch_size=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    eval_on_start=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2915d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test)\n",
    "\n",
    "logits = predictions.predictions\n",
    "labels = predictions.label_ids\n",
    "metrics = predictions.metrics\n",
    "\n",
    "pred_classes = np.argmax(logits, axis=-1)\n",
    "incorrect = test[labels != pred_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edb3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "for sample in test:\n",
    "    input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0).to(\"mps\")\n",
    "    attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0).to(\"mps\")\n",
    "    token_type_ids = torch.tensor(sample[\"token_type_ids\"]).unsqueeze(0).to(\"mps\")\n",
    "    label = sample[\"labels\"]\n",
    "\n",
    "    output = model(\n",
    "        input_ids=input_ids,\n",
    "        token_type_ids=token_type_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "197cf8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1562,  1.6087]], device='mps:0', grad_fn=<LinearBackward0>), hidden_states=(tensor([[[ 0.5589, -0.6705, -0.2784,  ...,  0.9875,  0.8205,  1.0542],\n",
       "         [-0.3003, -0.3265, -2.1491,  ..., -1.4600,  0.5116,  0.1713],\n",
       "         [ 0.1340, -0.9604,  0.3528,  ..., -0.8574, -0.4149,  0.7569],\n",
       "         ...,\n",
       "         [ 1.8806,  0.2689, -2.5269,  ..., -0.4895,  0.7896,  1.0888],\n",
       "         [ 0.3826, -0.2486, -1.4152,  ...,  1.3080,  0.6596,  1.2368],\n",
       "         [ 1.5368, -0.2942, -1.0405,  ...,  0.4380,  1.2213,  1.4403]]],\n",
       "       device='mps:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.1595, -1.0228, -0.5096,  ...,  1.4048,  0.3982,  1.2823],\n",
       "         [ 0.0996, -0.6921, -1.7849,  ..., -0.7312,  0.1989,  0.6718],\n",
       "         [ 0.6409, -1.2835, -0.0130,  ..., -0.1515, -0.6758,  0.8560],\n",
       "         ...,\n",
       "         [ 1.9764, -0.1478, -2.3243,  ..., -0.0387,  0.4647,  1.1858],\n",
       "         [ 0.7340, -0.6015, -1.2919,  ...,  1.4777,  0.2953,  1.5969],\n",
       "         [ 1.7871, -0.6006, -1.0921,  ...,  0.4873,  0.4767,  1.5038]]],\n",
       "       device='mps:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.7170, -1.5923, -0.3309,  ...,  0.9859,  0.6634,  1.1349],\n",
       "         [-0.0901, -0.8714, -1.3823,  ..., -0.4785,  0.4628,  0.6454],\n",
       "         [ 0.3206, -1.6477, -0.2096,  ..., -0.1249, -0.2473,  0.7634],\n",
       "         ...,\n",
       "         [ 1.2978, -0.7434, -1.7673,  ..., -0.1892,  0.7525,  1.2230],\n",
       "         [ 0.4672, -1.0099, -1.2472,  ...,  0.9189,  0.5256,  1.4160],\n",
       "         [ 1.2258, -1.0408, -1.0357,  ...,  0.1971,  0.7216,  1.2496]]],\n",
       "       device='mps:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.5141, -1.4440, -0.2719,  ...,  0.9089,  0.7984,  0.3178],\n",
       "         [-0.0238, -0.9807, -1.0917,  ..., -0.7321,  0.4390,  0.4659],\n",
       "         [ 0.4371, -1.5253, -0.2178,  ..., -0.3055, -0.0777,  0.5898],\n",
       "         ...,\n",
       "         [ 1.0260, -0.8684, -1.4746,  ..., -0.3251,  0.5521,  0.8012],\n",
       "         [ 0.3336, -0.9585, -1.0500,  ...,  0.5199,  0.4090,  0.8048],\n",
       "         [ 0.9522, -0.9550, -0.9241,  ..., -0.1103,  0.6001,  0.7581]]],\n",
       "       device='mps:0', grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1148, -0.8791, -0.5538,  ...,  0.5349,  0.6840, -0.6635],\n",
       "         [-0.0907, -0.9170, -1.0250,  ..., -0.2298,  0.4415, -0.4888],\n",
       "         [ 0.1070, -1.1086, -0.5841,  ..., -0.0473,  0.3363, -0.4527],\n",
       "         ...,\n",
       "         [ 0.4014, -0.7773, -1.1122,  ..., -0.0822,  0.5886, -0.3553],\n",
       "         [ 0.0377, -0.8242, -0.9957,  ...,  0.3483,  0.5503, -0.3277],\n",
       "         [ 0.3906, -0.8174, -0.9131,  ...,  0.0671,  0.5725, -0.3976]]],\n",
       "       device='mps:0', grad_fn=<NativeLayerNormBackward0>)), attentions=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rubiks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
